---
title: "HW1 Answer Key"
author: "Christian Baehr"
date: "2/10/2025"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load in required packages.
```{r echo = TRUE}
rm(list=ls())
pacman::p_load(quanteda, quanteda.corpora, quanteda.textstats, dplyr, ggplot2, 
               readtext, stringr, gutenbergr, stylest2, text.alignment)

```

## Question 1

```{r}

load("hw1/unga_speech_corpus.RData")

doc1 <- which(txtstatesyears$year==2017 & txtstatesyears$country=="United States of America")[1]
doc2 <- which(txtstatesyears$year==2017 & txtstatesyears$country=="United Kingdom")[1]
doc3 <- which(txtstatesyears$year==2017 & txtstatesyears$country=="Australia")[1]
txtstatesyears <- txtstatesyears[c(doc1, doc2, doc3) , ]

txtstatesyears$country[which(txtstatesyears$country=="United States of America")] <- "US"
txtstatesyears$country[which(txtstatesyears$country=="United Kingdom")] <- "UK"

## convert to corpus, with "text" as the variable with text data
unga.corpus <- corpus(txtstatesyears, text_field = "text", docid_field = "country")

## tokenize the speeches (no pre processing yet)
unga.tokens <- tokens(unga.corpus)

```

### 1a)

```{r}
## function to compute TTR
##
## @param x tokenized quanteda corpus
calculate_TTR <- function(x){
  ntype(x)/lengths(x)
}
calculate_TTR(unga.tokens)

## function to compute Guiraud's index of lexical richness
##
## @param x tokenized quanteda corpus
calculate_G <- function(x){
  ntype(x)/sqrt(lengths(x))
}
calculate_G(unga.tokens)

```

### 1b) 

```{r}

## create dfm of the UNGA speeches
unga.dfm <- tokens(unga.corpus, remove_punct = T) |>
  dfm(tolower=F)
textstat_simil(unga.dfm, margin = "documents", method = "cosine")

```

## Question 2

### 2a) Calculating TTR & Similarity w/ Stemming

```{r}
## Processing
unga.tokens <- tokens(unga.corpus, remove_punct = T) |>
  tokens_wordstem()

## TTR
ttr <- calculate_TTR(unga.tokens) %>% setNames(c("USA", "UK", "France"))
r <- calculate_G(unga.tokens) %>% setNames(c("USA", "UK", "France"))

## Similarity
unga.dfm <- dfm(unga.tokens, tolower = F)
sim <- textstat_simil(unga.dfm, margin = "documents", method = "cosine")

## print results
cat("TTR scores w. stemming: \n", ttr, "\n\n")
cat("G scores w. stemming: \n", r, "\n\n")
cat("Cosine similarity w. stemming: \n"); prmatrix(as.matrix(sim))
```


### 2b) Calculating TTR & Similarity w/o Stopwords 

```{r}
## Processing
unga.tokens <- tokens(unga.corpus, remove_punct = T) |>
  tokens_remove(stopwords("english"))

## TTR
ttr <- calculate_TTR(unga.tokens)
r <- calculate_G(unga.tokens) |>
  setNames(c("US", "UK", "France"))

## Similarity
unga.dfm <- dfm(unga.tokens, tolower = F)
sim <- textstat_simil(unga.dfm, margin = "documents", method = "cosine")

## print results
cat("TTR scores w/o stopwords: \n", ttr, "\n\n")
cat("G scores w/o stopwords: \n", r, "\n\n")
cat("Cosine similarity w/o stopwords: \n"); prmatrix(as.matrix(sim))
```

### 2c) Calculating TTR & Similarity w/ all lowercase 

```{r}
## Processing
unga.tokens <- tokens(unga.corpus, remove_punct = T) |>
  tokens_tolower()

## TTR
ttr <- calculate_TTR(unga.tokens)
r <- calculate_G(unga.tokens) 

## Similarity
unga.dfm <- dfm(unga.tokens)
sim <- textstat_simil(unga.dfm, margin = "documents", method = "cosine")

# print results
cat("TTR scores w. lowercase: \n", ttr, "\n\n")
cat("G scores w. lowercase: \n", r, "\n\n")
cat("Cosine similarity w. lowercases: \n"); prmatrix(as.matrix(sim))
```

### 2d) TF-IDF

```{r}
unga.dfm.tfidf <- tokens(unga.corpus, remove_punct = T) |>
  dfm() |>
  dfm_tfidf()

textstat_simil(unga.dfm.tfidf, margin = "documents", method = "cosine")
```

## Question 3

### 3a)

```{r}
## file names
files <- c("hw1/wealth_of_nations.txt", "hw1/theory_moral_sentiments.txt")

## read each text as a corpus object
smith <- readtext(files) |>
  corpus()

## docvar with titles
smith$title <- c("theory", "wealth")
```

### 3b) 

```{r}
## remove symbols/punctuation/numbers/stopwords, lowercase and remove hyphens
smith.tok <- smith |>
  tokens(remove_symbols = T, remove_punct = T, remove_numbers = T, split_hyphens = T) |>
  tokens_tolower() |>
  tokens_remove(stopwords())
```

### 3c)

```{r}
## use tfidf weighting with numerator the proportion of 
## document tokens of that type
smith.dfm <- dfm(smith.tok) |>
  dfm_tfidf(scheme_tf = "prop", base = exp(1))

topfeatures(dfm_subset(smith.dfm, title=="theory"), 15) # pretty close!

```

## Question 4

```{r}

sentence1 <- "Trump’s immigration crackdown sparks humanitarian crisis at the US border"
sentence2 <- "Trump’s immigration reforms strengthen US national security and US economy"

## remove punctuation and tokenize
sentences.tokens <- corpus(c(sentence1, sentence2)) |>
  tokens(remove_punct = T)

sentences.dfm <- dfm(sentences.tokens, tolower = T) 

s1 <- as.matrix(sentences.dfm)[1,] # feature vector for sentence1
s2 <- as.matrix(sentences.dfm)[2,] # feature vector for sentence2

## Euclidean distance
euclidean <- sqrt( sum( ( s1-s2 )^2 ) )

## Manhattan distance
manhattan <- sum( abs( s1-s2 ) )

## Jaccard distance
num <- length( intersect(sentences.tokens[[1]], sentences.tokens[[2]]) )
denom <- length( union(sentences.tokens[[1]],sentences.tokens[[2]]) )
jaccard <- num / denom

## Cosine similarity
cosine <- sum(s1 * s2) /( sqrt(sum(s1^2)) * sqrt(sum(s2^2)) )

## Levenshtein distance for surveyance and surveillance
text1 <- "At the theatre, my neighbour wore her favourite jewellery"
text2 <- "At the theater, my neighbor wore her favorite jewelry"
levenshtein <- adist(text1, text2)

dl <- stringdist::stringdist(text1, text2, method="dl")

## print
cat("Euclidean distance:", euclidean, "\n\n",
    "Manhattan distance:", manhattan, "\n\n",
    "Jaccard similarity:", jaccard, "\n\n",
    "Cosine similarity:", cosine, "\n\n",
    "Levenshtein distance:", levenshtein, "\n\n",
    "Damerau-Levenshtein distance:", dl)

```

## Question 5

### 5a) Contingency table for UK Manifestos

```{r}
## get text from UK political manifestos speeches
corpus <- corpus_subset(data_corpus_ukmanifestos)
text <- tokens(corpus, remove_punct = T) |>
  tokens_tolower() |>
  paste(collapse = " ")
  
## get entry of contingency table for the collocation
o11 <- str_count(text, "northern(?= ireland)")
o12 <- str_count(text, "northern(?! ireland)")
o21 <- str_count(text, "(?<!northern )ireland")
N <- tokens(text) |>
  tokens_ngrams(n = 2) |>
  ntoken() |>
  unname()
o22 <- N - o21 - o11 - o12

## contingency table
out <- matrix(c(o11, o12, o21, o22),
                 ncol = 2,
                 byrow = T)
rownames(out) <- c("Northern", "Not Northern")
colnames(out) <- c("Ireland", "Not Ireland")
print(out)

## expected frequency
E11 <- (o11+o12)/N * (o11 + o21)/N * N
# N12 <- N - (o11 + o21)
# E21 <- (o11+o21)/N * N21/N * N
# E12 <- (o11+o12)/N * N12/N * N
# E22 <- N12/N * N21/N * N

## get Chi-square value
## (o11-E11)^2/E11 + (o21-E21)^2/E21 + (o12-E12)^2/E12 + (o22-E22)^2/E22

## print
cat("Observed frequency:", o11, "\n\n",
    "Expected frequency:", E11)
```

### 5b) Collocation for "Northern Ireland" using quanteda

```{r}
textstat_collocations(corpus, min_count = 5) |>
  data.frame() |>
  select(c("collocation", "lambda", "z")) |> 
  filter(collocation == "northern ireland") 
```

### 5c) Collocations using quanteda

```{r}
(collout1 <- textstat_collocations(corpus, min_count = 5) |>
   arrange(-lambda) |>
   slice(1:10) |>
   data.frame() |>
   select(c("collocation", "count", "lambda", "z")))

(collout2 <- textstat_collocations(corpus, min_count = 5) |>
   arrange(-count) |>
   slice(1:10) |>
   data.frame() |>
   select(c("collocation", "count", "lambda", "z")))
```

## Question 6

```{r}
dfm <- smith |>
  corpus_subset(title=="theory") |>
  tokens(remove_punct = T) |>
  dfm()

## regression to check if slope is approx -1.0
regression <- lm(log(topfeatures(dfm, 100)) ~ log(1:100))
summary(regression)
confint(regression)

# create plot to illustrate zipf's law
plot(log(1:100), log(topfeatures(dfm, 100)),
     xlab="log(rank)", ylab="log(frequency)", main="Top 100 Words")
abline(regression, col="red")
abline(a = regression$coefficients["(Intercept)"], b = -1, col = "black")
```

## Question 7

```{r}
## Heap's Law
## M = kT^b
## where:
## M = vocab size
## T = number of tokens
## k, b are constants

dfm <- smith |>
  corpus_subset(title=="theory") |>
  tokens(remove_punct = T) |>
  dfm()

num_tokens <- sum(rowSums(dfm))
M <- nfeat(dfm)
k <- 44

## solve for b
b <- log(M/k)/log(num_tokens)
print(b)

## Now without lowercase

dfm <- smith |>
  corpus_subset(title=="theory") |>
  tokens(remove_punct = T) |>
  dfm(tolower=F)

num_tokens <- sum(rowSums(dfm))
M <- nfeat(dfm)
k <- 44

## solve for b
b <- log(M/k)/log(num_tokens)
print(b)

```

## Question 8

```{r eval = F}
corpus <- txtstatesyears |>
  filter(country %in% c("United States of America", "China")) |>
  corpus(text_field="text")

## key words in context
corpus_subset(corpus, country == "United States of America") |>
  tokens(remove_punct = T) |>
  kwic("nation", window = 5)
corpus_subset(corpus, country == "United States of America") |>
  tokens(remove_punct = T) |>
  kwic("industry", window = 5)

corpus_subset(corpus, country == "China") |>
  tokens(remove_punct = T) |>
  kwic("nation", window = 5)
corpus_subset(corpus, country == "China") |> 
  tokens(remove_punct = T) |>
  kwic("industry", window = 5)
```

## Question 9

### 9a)

```{r}
load("hw1/unga_speech_corpus.RData")

unga.df.sub <- txtstatesyears |>
  filter(country=="United States of America" & year %in% c(2005:2015)) 
unga.sub <- corpus(unga.df.sub, text_field="text")
docvars(unga.sub, "year") <- unga.df.sub$year

unga.sub <- unga.sub |>
  corpus_reshape("sentence")

unga.df <- cbind(as.character(unga.sub), docvars(unga.sub)["year"]) |>
  setNames(c("text", "year"))

unga.split <- split(unga.df, as.factor(unga.df$year))


boot.fre <- function(year) { # accepts df of texts (year-specific)
  n <- nrow(year) # number of texts
  docnums <- sample(1:n, size=n, replace=T) # sample texts WITH replacement
  docs.boot <- corpus(year[docnums, "text"])
  docnames(docs.boot) <- 1:length(docs.boot) # something you have to do
  fre <- textstat_readability(docs.boot, measure = "Flesch") # compute FRE for each
  return(mean(fre[,"Flesch"])) # return flesch scores only
}

lapply(unga.split, boot.fre) # apply to each df of party texts

iter <- 10 # NUMBER OF BOOTSTRAP SAMPLES (usually would want more, >=100)

## for loop to compute as many samples as specified
for(i in 1:iter) {
  if(i==1) {boot.means <- list()} # generate new list
  
  # store the results in new element i
  boot.means[[i]] <- lapply(unga.split, boot.fre) 
  print(paste("Iteration", i))
}

## combine the point estimates to a data frame and compute statistics by party
boot.means.df <- do.call(rbind.data.frame, boot.means)
mean.boot <- apply(boot.means.df, 2, mean)
sd.boot <- apply(boot.means.df, 2, sd)

## create data frame for plot
plot_df <- data.frame(sort(unique(unga.df$year)), mean.boot, sd.boot) |>
  setNames(c("year", "mean", "se"))

## confidence intervals
ci90 <- qnorm(0.95) 
ci95 <- qnorm(0.975)

## ggplot point estimate + variance
ggplot(plot_df, aes(colour = year)) + # general setup for plot
  geom_linerange(aes(x = year, 
                     ymin = mean - se*ci90, 
                     ymax = mean + se*ci90), 
                 lwd = 1, position = position_dodge(width = 1/2)) + # plot 90% interval
  geom_pointrange(aes(x = year, 
                      y = mean, 
                      ymin = mean - se*ci95, 
                      ymax = mean + se*ci95), 
                  lwd = 1/2, position = position_dodge(width = 1/2), 
                  shape = 21, fill = "WHITE") + # plot point estimates and 95% interval
  #coord_flip() + # fancy stuff
  theme_bw() + # fancy stuff
  xlab("") + ylab("Mean Flesch Score, by Year") + # fancy stuff
  theme(legend.position = "none") # fancy stuff
```

### 9b)

```{r}
## mean Flesch statistic by year
flesch_point <- unga.df$text |> 
  textstat_readability(measure = "Flesch") |>
  group_by(unga.df$year) |>
  summarise(mean_flesch = mean(Flesch)) |> 
  setNames(c("year", "mean")) |>
  arrange(as.numeric(year))

cbind(flesch_point, "bs_mean" = plot_df$mean)
```

### 9c)

```{r}
## calculate the FRE score and the Dale-Chall score.
fre_and_dc_measures <- textstat_readability(unga.sub, c("Flesch", "FOG"))

## compute correlations
readability_cor <- cor(cbind(fre_and_dc_measures$Flesch, fre_and_dc_measures$FOG))

## print
print(readability_cor[1,2])
```

## Question 10

```{r}

docs <- corpus( readtext(c("hw1/data/jefferson_draft.txt", "hw1/data/final_version.txt")))

# set gap to default (-1)
sw2 <- smith_waterman(as.character(docs)[1], as.character(docs)[2], 
                      type="words", gap=-1)

# increase match, decrease mismatch --> increases extent of plagiarism. Why?
sw3 <- smith_waterman(as.character(docs)[1], as.character(docs)[2], 
                      type="words", 
                      match= 3,
                      mismatch = 0,
                      gap=-1)

print(sw2$sw)
print(sw3$sw)

```

## Question 11


```{r}

rc <- read.csv("hw1/countypres_2000-2020.csv")
rc20 <- rc[rc$year=="2020",]
rc20DR <- rc20[rc20$party%in%c("REPUBLICAN", "DEMOCRAT"),]

# look at specific state
# TX works

state <- "VT"
rc_state <- rc20DR[rc20DR$state_po==state,]

#look at leading digit v frequency
digits <- rc_state$candidatevotes
first_digits <-  as.numeric(substr(digits, 1, 1))

rc_state$first_digs <- first_digits

dems <- rc_state[rc_state$party=="DEMOCRAT",]
total_dems <- table(factor(dems$first_digs, levels=1:9) )

rep <-  rc_state[rc_state$party=="REPUBLICAN",]
total_reps <- table(factor(rep$first_digs, levels=1:9) )

#benford expectations
ben_props  <- c(0.301, .176, .125, .097, 0.079, 0.067, 0.058, 0.051, 0.046)
names(ben_props)<- c("1" ,"2", "3" ,"4" ,"5", "6", "7", "8", "9")

the_benprops <- ben_props*length(unique(rc_state$county_name))

# now do barplot
dat <- rbind(total_dems, the_benprops, total_reps)
rownames(dat) <- c("dem", "benford", "rep")
colnames(dat) <- c("1" ,"2", "3" ,"4" ,"5", "6", "7", "8", "9")

#x11()
barplot(dat, col = c("blue","black","red"), beside = T, main=state) 

# article: 
# https://www.reuters.com/article/world/fact-check-deviation-from-benfords-law-does-not-prove-election-fraud-idUSKBN27Q3A9/
# Election Integrity Partnership

# look at magnitude of districts
#x11()
hist(log(unique(rc_state$totalvotes)), main=state) # TX has broad range of magnitudes, VT does not


```